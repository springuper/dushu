# 数据提取与发布操作手册

> **快速参考指南**：本文档提供数据提取和发布的完整操作流程，适合日常使用。

---

## 📋 目录

1. [快速开始](#快速开始)
2. [完整流程](#完整流程)
3. [详细步骤](#详细步骤)
4. [常见问题](#常见问题)
5. [最佳实践](#最佳实践)

---

## 🚀 快速开始

### 最简单的流程（3 步）

```
1. 上传章节 → 2. 提取数据 → 3. 批量审核
```

**预计时间**：10-15 分钟（取决于章节大小和审核数据量）

### 完整流程（包含数据准备）

如果还没有章节 JSON 文件：

```
1. 下载原始文本（脚本） → 2. 预处理（脚本） → 3. 上传章节 → 4. 提取数据 → 5. 批量审核
```

**预计时间**：15-20 分钟

---

## 📖 完整流程

### 流程图

```
获取原始文本
    ↓
预处理（生成 JSON）
    ↓
上传章节到系统
    ↓
在章节处理页面提取数据
    ↓
批量审核（通过即发布）
    ↓
完成 ✅
```

---

## 🔧 详细步骤

### 步骤 1：准备章节数据

> **提示**：如果已经有预处理好的 JSON 文件，可以跳过步骤 1.1 和 1.2，直接进入步骤 2。

#### 1.1 获取原始文本

**方法 1：使用脚本自动下载（推荐）**

使用 Playwright 脚本自动从维基文库下载：

```bash
python scripts/download_with_playwright.py \
  --url "https://zh.wikisource.org/wiki/史記/卷008" \
  --output data/raw/shiji/shiji_01_gaozu_benji.txt \
  --book "史记" \
  --chapter "高祖本纪"
```

**参数说明**：
- `--url`: 维基文库页面 URL
- `--output`: 输出文件路径
- `--book`: 书籍名称（可选）
- `--chapter`: 章节名称（可选）

**前提条件**：
- 安装 Playwright：`pip install playwright`
- 安装浏览器：`playwright install chromium`

**快速示例**（下载《史记·高祖本纪》）：
```bash
# 使用自动脚本（推荐）
./scripts/download_first_chapter_auto.sh

# 或手动运行
python scripts/download_with_playwright.py \
  --url "https://zh.wikisource.org/wiki/史記/卷008" \
  --output data/raw/shiji/shiji_01_gaozu_benji.txt \
  --book "史记" \
  --chapter "高祖本纪"
```

**方法 2：手动下载**

从以下来源获取历史文本：
- **维基文库**：https://zh.wikisource.org
- **中国哲学书电子化计划**：https://ctext.org

手动复制内容并保存为 UTF-8 编码的文本文件，路径：`data/raw/{book}/{book}_{chapter}_{title}.txt`

**文件格式要求**：
文件开头应包含来源信息：
```
来源：《史记·高祖本纪》
获取渠道：维基文库
URL：https://zh.wikisource.org/wiki/史記/卷008
获取日期：2024-12-08
版权状态：公共领域（Public Domain）
```

#### 1.2 预处理（生成结构化 JSON）

使用预处理脚本将原始文本转换为结构化 JSON：

```bash
python scripts/preprocess_text.py \
  --input data/raw/shiji/shiji_01_gaozu_benji.txt \
  --output data/processed/chapters/shiji_01_gaozu_benji.json \
  --book "史记" \
  --chapter "高祖本纪" \
  --url "https://zh.wikisource.org/wiki/史記/卷008"
```

**参数说明**：
- `--input`: 原始文本文件路径
- `--output`: 输出 JSON 文件路径
- `--book`: 书籍名称（可选）
- `--chapter`: 章节名称（可选）
- `--url`: 来源 URL（可选）

**输出格式**：

**输出格式**：
```json
{
  "title": "高祖本纪",
  "source": {
    "book": "史记",
    "chapter": "高祖本纪",
    "url": "https://zh.wikisource.org/wiki/史記/卷008"
  },
  "paragraphs": [
    {
      "order": 1,
      "text": "段落内容...",
      "id": "para_1"
    }
  ]
}
```

**一键完成步骤 1.1 和 1.2**：

如果使用自动脚本，可以一次性完成下载和预处理：

```bash
# 下载并预处理《史记·高祖本纪》
./scripts/download_first_chapter_auto.sh
```

这个脚本会：
1. 自动下载原始文本
2. 自动预处理生成 JSON
3. 保存到 `data/processed/chapters/` 目录

---

### 步骤 2：上传章节

#### 2.1 创建书籍（如果还没有）

1. 登录管理后台
2. 进入 **内容管理 → 书籍管理**
3. 点击 **新建书籍**
4. 填写书籍信息：
   - 书名：如"史记"
   - 英文名：如"shiji"
   - 作者：如"司马迁"
   - 朝代：如"西汉"
   - 其他信息（可选）
5. 点击 **保存**

#### 2.2 上传章节 JSON

1. 进入 **内容管理 → 书籍章节**
2. 选择 **导入章节** 标签页
3. 选择书籍（从下拉框选择）
4. 选择章节 JSON 文件
5. 点击 **开始导入**

**注意**：
- 章节会保存到数据库
- 段落会自动创建
- 此时还没有提取数据

---

### 步骤 3：提取数据

#### 3.1 进入章节处理页面

1. 进入 **内容管理 → 章节处理**
2. 选择书籍（从下拉框选择）
3. 选择章节（从下拉框选择）

#### 3.2 提取模式（混合，事件为中心）
- 系统默认使用“事件联合抽取 + 人物/地点补全 + 对齐消歧”，无需勾选类型。
- 每批事件上限 30，人物/地点上限 60；超出会按重要性截断并在结果中标记长尾。
- 文本分段：约 4k-6k tokens（长章 8k-12k），防止超长输入稀释注意力。
- 默认模型：Gemini `gemini-2.5-flash`，温度 0.3。

#### 3.3 开始提取

1. 点击 **开始提取** 按钮
2. 等待提取完成（显示 loading 状态）
3. 查看提取结果：
   - 人物：X 条
   - 关系：X 条
   - 地点：X 条
   - 事件：X 条

**注意**：
- 提取是同步的，可能需要几分钟（取决于章节大小）
- 提取结果会自动创建 ReviewItem，状态为 `PENDING`
- 如果提取失败，会显示错误信息，可以重试；失败的分段/长尾会保留标记，便于后续补抽

---

### 步骤 4：批量审核

#### 4.1 在章节处理页面审核

1. 在章节处理页面下方查看 **待审核数据** 列表
2. 勾选要审核的数据（可以全选）
3. 点击 **批量通过** 或 **批量拒绝**

**批量通过**：
- 系统会自动判断是否重复
- 如果检测到重复，会使用 LLM 融合
- 通过后数据状态直接设为 `PUBLISHED`（已发布）

**批量拒绝**：
- 标记为 `REJECTED`
- 可以填写拒绝原因

#### 4.2 在 Review 页面审核（可选）

1. 进入 **数据准备 → Review**
2. 筛选待审核数据（状态：待审核）
3. 勾选要审核的数据
4. 点击 **批量通过** 或 **批量拒绝**

**优势**：
- 可以按类型筛选（人物、关系、地点、事件）
- 可以查看详细信息
- 支持单个审核

---

### 步骤 5：完成 ✅

审核通过后：
- ✅ 数据状态为 `PUBLISHED`
- ✅ 可以直接在系统中使用
- ✅ 可以在人物、关系、地点、事件页面查看

---

## ❓ 常见问题

### Q1: 如何快速获取章节数据？

**推荐方法**：
使用自动脚本一键完成下载和预处理：

```bash
# 下载《史记·高祖本纪》并预处理
./scripts/download_first_chapter_auto.sh
```

**手动方法**：
1. 使用下载脚本：`python scripts/download_with_playwright.py --url "..." --output "..."`
2. 使用预处理脚本：`python scripts/preprocess_text.py --input "..." --output "..."`

### Q2: 提取失败怎么办？

**可能原因**：
- LLM API 配置错误
- 网络问题
- 章节文本过长

**解决方法**：
1. 检查环境变量（`OPENAI_API_KEY` 或 `GOOGLE_API_KEY`）
2. 检查网络连接
3. 重试提取
4. 如果章节过长，可以分段处理

### Q3: 如何清理旧的提取数据？

**操作**：
- 运行清理脚本（需要 ts-node）：  
  ```bash
  npx ts-node backend/scripts/cleanup_old_extraction.ts
  ```
- 将删除 `source` 为 `LLM_EXTRACT` 的 ReviewItem；已发布数据不受影响。

### Q4: 提取时间太长怎么办？

**说明**：
- 大章节提取可能需要 5-10 分钟
- 这是正常的，因为需要调用 LLM API
- 前端会显示 loading 状态

**建议**：
- 耐心等待
- 不要关闭页面
- 如果超时，可以重试

### Q5: 审核通过后数据在哪里？

**答案**：
- 人物数据：**内容管理 → 人物管理**
- 关系数据：**内容管理 → 关系管理**
- 地点数据：**内容管理 → 地点管理**
- 事件数据：**内容管理 → 事件管理**

所有数据状态为 `PUBLISHED`，可以直接使用。

### Q6: 如何修改已发布的数据？

**方法**：
1. 进入对应的管理页面（如人物管理）
2. 找到要修改的数据
3. 点击编辑按钮
4. 修改后保存

**注意**：修改会记录到变更日志中。

### Q7: 批量审核时如何知道哪些数据会合并？

**说明**：
- 系统会自动检测重复
- 如果检测到重复，会在 ReviewItem 中标记
- 批量通过时会自动使用 LLM 融合
- 融合结果会记录到变更日志中

**建议**：
- 批量通过前可以先查看单个 ReviewItem 的详情
- 确认融合结果后再批量操作

---

## 💡 最佳实践

### 1. 工作流程建议

**推荐流程**：
1. 一次上传一个章节
2. 提取后立即审核
3. 审核通过后再处理下一个章节

**优势**：
- 避免数据堆积
- 及时发现和解决问题
- 保持数据质量

### 3. 提取模式选择

- 默认使用“事件联合 + 人物/地点补全 + 对齐消歧”混合模式，无需手动勾选类型。
- 每批事件≤30，人物/地点≤60；截断的长尾会被标记，后续可单独补抽。
- 如果章节极短，可接受一次性输出；长章建议保持分段，便于并行与重试。

### 4. 批量审核策略

**小批量审核**（推荐）：
- 每次审核 10-20 条数据
- 可以仔细检查每条数据
- 减少错误率

**大批量审核**：
- 适合数据质量较高的情况
- 可以快速处理大量数据
- 但需要仔细检查融合结果

### 5. 数据质量检查

**审核时注意**：
- ✅ 人物姓名是否正确
- ✅ 关系类型是否准确
- ✅ 地点坐标是否合理
- ✅ 事件时间是否准确
- ✅ 融合结果是否合理

### 6. 错误处理

**如果发现错误**：
1. 拒绝错误的 ReviewItem
2. 在对应的管理页面修改已发布的数据
3. 或者删除错误数据后重新提取

---

## 📊 数据统计

### 查看提取结果

在章节处理页面可以看到：
- 人物数量
- 关系数量
- 地点数量
- 事件数量

### 查看审核状态

在 Review 页面可以：
- 查看待审核数量
- 查看已通过数量
- 查看已拒绝数量

---

## 🔗 相关文档

- [数据获取与融合规格书](../specs/data-acquisition-and-merge-spec.md) - 详细技术规范
- [快速开始指南](./setup/QUICK_START.md) - 环境设置
- [数据来源说明](./data/DATA_SOURCES.md) - 数据来源和版权

---

## 📝 更新日志

### v2.0 (2024-12-08)

- ✅ 新增章节处理页面，统一管理章节相关操作
- ✅ 支持在系统中直接提取数据，无需线下脚本
- ✅ 简化审核流程，审核通过即发布
- ✅ 支持批量审核，提高操作效率

---

**最后更新**：2024-12-08  
**版本**：2.0

